module attributes {torch.debug_module_name = "DLRM_Net"} {
  func.func @forward(%arg0: !torch.vtensor<[1,4],f32>, %arg1: !torch.vtensor<[3,1],si64>, %arg2: !torch.vtensor<[3],si64>, %arg3: !torch.vtensor<[1],si64>, %arg4: !torch.vtensor<[1],si64>) -> !torch.vtensor<[1,1],f32> {
    %int3 = torch.constant.int 3
    %int2 = torch.constant.int 2
    %int1 = torch.constant.int 1
    %0 = torch.vtensor.literal(dense<[[-0.886635601, -0.59805268]]> : tensor<1x2xf32>) : !torch.vtensor<[1,2],f32>
    %1 = torch.vtensor.literal(dense<-1.2125231> : tensor<1xf32>) : !torch.vtensor<[1],f32>
    %2 = torch.vtensor.literal(dense<[[0.600286722, -0.232883498, -0.072763212, -0.483540505], [-0.927203059, 0.724711657, -0.397718698, 0.958951354]]> : tensor<2x4xf32>) : !torch.vtensor<[2,4],f32>
    %3 = torch.vtensor.literal(dense<[0.570853114, -0.222567618]> : tensor<2xf32>) : !torch.vtensor<[2],f32>
    %4 = torch.vtensor.literal(dense<[[0.115790367, -0.328789502, -0.705318093, -0.15958418, 0.234255269, 0.138228402, -0.00482977927, 9.766790e-01], [0.168570682, 0.399567306, 0.913718163, -0.528308094, -0.424083501, 0.711867511, -0.32580775, 0.0121181281], [0.436546415, 0.363629371, 0.716429293, 0.610594153, 0.436577737, -0.315457016, 0.324501336, 0.12830101], [-0.541445613, 0.578609884, 0.329552948, 0.0185712483, -0.0951594337, -0.489204407, 0.0814553648, 0.191239476]]> : tensor<4x8xf32>) : !torch.vtensor<[4,8],f32>
    %5 = torch.vtensor.literal(dense<[-0.415577501, 5.811020e-01, -0.548601508, -1.06155014]> : tensor<4xf32>) : !torch.vtensor<[4],f32>
    %6 = torch.vtensor.literal(dense<[0, 0, 1, 0, 1, 2]> : tensor<6xsi64>) : !torch.vtensor<[6],si64>
    %7 = torch.vtensor.literal(dense<[1, 2, 2, 3, 3, 3]> : tensor<6xsi64>) : !torch.vtensor<[6],si64>
    %8 = torch.vtensor.literal(dense<[[-0.144187197, 0.336576343], [-0.449024498, -0.458980531]]> : tensor<2x2xf32>) : !torch.vtensor<[2,2],f32>
    %9 = torch.vtensor.literal(dense<[[-0.0220179427, -0.124571957], [-0.181082428, 0.264483809], [-0.0709306598, -0.508440197]]> : tensor<3x2xf32>) : !torch.vtensor<[3,2],f32>
    %10 = torch.vtensor.literal(dense<[[0.196469188, -0.213860661], [-0.273148537, 0.0513147675], [0.219468966, -0.0768935382], [0.48076421, 0.184829742]]> : tensor<4x2xf32>) : !torch.vtensor<[4,2],f32>
    %11 = torch.vtensor.literal(dense<[[-1.76998317, -1.12041593, -0.442641228], [0.586578727, -0.109816849, 0.00179991522]]> : tensor<2x3xf32>) : !torch.vtensor<[2,3],f32>
    %12 = torch.vtensor.literal(dense<[0.48664695, -0.621926128]> : tensor<2xf32>) : !torch.vtensor<[2],f32>
    %13 = torch.vtensor.literal(dense<[[1.17911923, 1.1688863, 0.536689401, 0.206425309], [0.394140095, 0.79682976, -0.500224233, 0.628507077], [-0.670227408, -0.340892524, 0.484868109, -0.76366198]]> : tensor<3x4xf32>) : !torch.vtensor<[3,4],f32>
    %14 = torch.vtensor.literal(dense<[-0.0808687135, -0.497534424, -0.147581905]> : tensor<3xf32>) : !torch.vtensor<[3],f32>
    %int0 = torch.constant.int 0
    %int-1 = torch.constant.int -1
    %int4 = torch.constant.int 4
    %false = torch.constant.bool false
    %none = torch.constant.none
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %true = torch.constant.bool true
    %15 = torch.tensor_static_info_cast %arg1 : !torch.vtensor<[3,1],si64> to !torch.vtensor<*,si64>
    %16 = torch.copy.to_tensor %15 : !torch.tensor<*,si64>
    %17 = torch.tensor_static_info_cast %arg2 : !torch.vtensor<[3],si64> to !torch.vtensor<*,si64>
    %18 = torch.copy.to_tensor %17 : !torch.tensor<*,si64>
    %19 = torch.tensor_static_info_cast %arg3 : !torch.vtensor<[1],si64> to !torch.vtensor<*,si64>
    %20 = torch.copy.to_tensor %19 : !torch.tensor<*,si64>
    %21 = torch.tensor_static_info_cast %arg4 : !torch.vtensor<[1],si64> to !torch.vtensor<*,si64>
    %22 = torch.copy.to_tensor %21 : !torch.tensor<*,si64>
    %23 = torch.tensor_static_info_cast %16 : !torch.tensor<*,si64> to !torch.tensor<[3,1],si64>
    %24 = torch.aten.linear %arg0, %13, %14 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[3,4],f32>, !torch.vtensor<[3],f32> -> !torch.vtensor<[1,3],f32>
    %25 = torch.aten.relu %24 : !torch.vtensor<[1,3],f32> -> !torch.vtensor<[1,3],f32>
    %26 = torch.aten.linear %25, %11, %12 : !torch.vtensor<[1,3],f32>, !torch.vtensor<[2,3],f32>, !torch.vtensor<[2],f32> -> !torch.vtensor<[1,2],f32>
    %27 = torch.aten.relu %26 : !torch.vtensor<[1,2],f32> -> !torch.vtensor<[1,2],f32>
    %28 = torch.aten.slice.Tensor %23, %int0, %int0, %int1, %int1 : !torch.tensor<[3,1],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1],si64>
    %29 = torch.aten.squeeze.dim %28, %int0 : !torch.tensor<[1,1],si64>, !torch.int -> !torch.tensor<[1],si64>
    %30 = torch.tensor_static_info_cast %18 : !torch.tensor<*,si64> to !torch.tensor<[3],si64>
    %31 = torch.copy.to_tensor %10 : !torch.tensor<[4,2],f32>
    %32:4 = torch.operator "aten.embedding_bag.padding_idx"(%31, %30, %29, %false, %int0, %true, %none, %false, %none) : (!torch.tensor<[4,2],f32>, !torch.tensor<[3],si64>, !torch.tensor<[1],si64>, !torch.bool, !torch.int, !torch.bool, !torch.none, !torch.bool, !torch.none) -> (!torch.tensor<[1,2],f32>, !torch.tensor<[0],si64>, !torch.tensor<[1],si64>, !torch.tensor<[1],si64>)
    %33 = torch.aten.slice.Tensor %23, %int0, %int1, %int2, %int1 : !torch.tensor<[3,1],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1],si64>
    %34 = torch.aten.squeeze.dim %33, %int0 : !torch.tensor<[1,1],si64>, !torch.int -> !torch.tensor<[1],si64>
    %35 = torch.tensor_static_info_cast %20 : !torch.tensor<*,si64> to !torch.tensor<[1],si64>
    %36 = torch.copy.to_tensor %9 : !torch.tensor<[3,2],f32>
    %37:4 = torch.operator "aten.embedding_bag.padding_idx"(%36, %35, %34, %false, %int0, %true, %none, %false, %none) : (!torch.tensor<[3,2],f32>, !torch.tensor<[1],si64>, !torch.tensor<[1],si64>, !torch.bool, !torch.int, !torch.bool, !torch.none, !torch.bool, !torch.none) -> (!torch.tensor<[1,2],f32>, !torch.tensor<[0],si64>, !torch.tensor<[1],si64>, !torch.tensor<[1],si64>)
    %38 = torch.aten.slice.Tensor %23, %int0, %int2, %int3, %int1 : !torch.tensor<[3,1],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.tensor<[1,1],si64>
    %39 = torch.aten.squeeze.dim %38, %int0 : !torch.tensor<[1,1],si64>, !torch.int -> !torch.tensor<[1],si64>
    %40 = torch.tensor_static_info_cast %22 : !torch.tensor<*,si64> to !torch.tensor<[1],si64>
    %41 = torch.copy.to_tensor %8 : !torch.tensor<[2,2],f32>
    %42:4 = torch.operator "aten.embedding_bag.padding_idx"(%41, %40, %39, %false, %int0, %true, %none, %false, %none) : (!torch.tensor<[2,2],f32>, !torch.tensor<[1],si64>, !torch.tensor<[1],si64>, !torch.bool, !torch.int, !torch.bool, !torch.none, !torch.bool, !torch.none) -> (!torch.tensor<[1,2],f32>, !torch.tensor<[0],si64>, !torch.tensor<[1],si64>, !torch.tensor<[1],si64>)
    %43 = torch.prim.NumToTensor.Scalar %int1 : !torch.int -> !torch.vtensor<[],si64>
    %44 = torch.prim.NumToTensor.Scalar %int2 : !torch.int -> !torch.vtensor<[],si64>
    %45 = torch.copy.to_vtensor %32#0 : !torch.vtensor<[1,2],f32>
    %46 = torch.copy.to_vtensor %37#0 : !torch.vtensor<[1,2],f32>
    %47 = torch.copy.to_vtensor %42#0 : !torch.vtensor<[1,2],f32>
    %48 = torch.prim.ListConstruct %27, %45, %46, %47 : (!torch.vtensor<[1,2],f32>, !torch.vtensor<[1,2],f32>, !torch.vtensor<[1,2],f32>, !torch.vtensor<[1,2],f32>) -> !torch.list<vtensor>
    %49 = torch.aten.cat %48, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[1,8],f32>
    %50 = torch.prim.ListConstruct %int1, %int-1, %int2 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %51 = torch.aten.view %49, %50 : !torch.vtensor<[1,8],f32>, !torch.list<int> -> !torch.vtensor<[1,4,2],f32>
    %52 = torch.aten.transpose.int %51, %int1, %int2 : !torch.vtensor<[1,4,2],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,2,4],f32>
    %53 = torch.aten.bmm %51, %52 : !torch.vtensor<[1,4,2],f32>, !torch.vtensor<[1,2,4],f32> -> !torch.vtensor<[1,4,4],f32>
    %54 = torch.copy.to_tensor %7 : !torch.tensor<[6],si64>
    %cpu = torch.constant.device "cpu"
    %55 = torch.operator "aten.to.device"(%54, %cpu, %int4, %false, %false, %none) : (!torch.tensor<[6],si64>, !torch.Device, !torch.int, !torch.bool, !torch.bool, !torch.none) -> !torch.tensor<[6],si64>
    %56 = torch.aten.detach %55 : !torch.tensor<[6],si64> -> !torch.tensor<[6],si64>
    %57 = torch.copy.to_tensor %6 : !torch.tensor<[6],si64>
    %58 = torch.operator "aten.to.device"(%57, %cpu, %int4, %false, %false, %none) : (!torch.tensor<[6],si64>, !torch.Device, !torch.int, !torch.bool, !torch.bool, !torch.none) -> !torch.tensor<[6],si64>
    %59 = torch.aten.detach %58 : !torch.tensor<[6],si64> -> !torch.tensor<[6],si64>
    %60 = torch.aten.slice.Tensor %53, %int0, %int0, %int9223372036854775807, %int1 : !torch.vtensor<[1,4,4],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,4,4],f32>
    %61 = torch.copy.to_vtensor %56 : !torch.vtensor<[6],si64>
    %62 = torch.copy.to_vtensor %59 : !torch.vtensor<[6],si64>
    %63 = torch.prim.ListConstruct %none, %61, %62 : (!torch.none, !torch.vtensor<[6],si64>, !torch.vtensor<[6],si64>) -> !torch.list<optional<vtensor>>
    %64 = torch.aten.index.Tensor %60, %63 : !torch.vtensor<[1,4,4],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[1,6],f32>
    %65 = torch.prim.ListConstruct %27, %64 : (!torch.vtensor<[1,2],f32>, !torch.vtensor<[1,6],f32>) -> !torch.list<vtensor>
    %66 = torch.aten.cat %65, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[1,8],f32>
    %67 = torch.aten.linear %66, %4, %5 : !torch.vtensor<[1,8],f32>, !torch.vtensor<[4,8],f32>, !torch.vtensor<[4],f32> -> !torch.vtensor<[1,4],f32>
    %68 = torch.aten.relu %67 : !torch.vtensor<[1,4],f32> -> !torch.vtensor<[1,4],f32>
    %69 = torch.aten.linear %68, %2, %3 : !torch.vtensor<[1,4],f32>, !torch.vtensor<[2,4],f32>, !torch.vtensor<[2],f32> -> !torch.vtensor<[1,2],f32>
    %70 = torch.aten.relu %69 : !torch.vtensor<[1,2],f32> -> !torch.vtensor<[1,2],f32>
    %71 = torch.aten.linear %70, %0, %1 : !torch.vtensor<[1,2],f32>, !torch.vtensor<[1,2],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1,1],f32>
    %72 = torch.aten.sigmoid %71 : !torch.vtensor<[1,1],f32> -> !torch.vtensor<[1,1],f32>
    return %72 : !torch.vtensor<[1,1],f32>
  }
}
